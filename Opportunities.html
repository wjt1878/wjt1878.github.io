<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Generic Page - Massively by HTML5 UP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="active"><a href="index.html">Home</a></li>
							<li class="active"><a href="XAI.html">XAI</a></li>
							<li class="active"><a href="Opportunities.html">Opportunities</a></li>
							<li class="active"><a href="Risks.html">Risks</a></li>
							<li class="active"><a href="Choices.html">Choices</a></li>
							<li class="active"><a href="Ethics.html">Ethics</a></li>
							<li class="active"><a href="References.html">References</a></li>
							<li class="active"><a href="Process Support.html">Process Support</a></li>	
						</ul>
						<ul class="icons">
							<li><a href="#" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<span class="date">April 25, 2017</span> 
									<h1>Opportunities</h1> 
									<p>Discover how Explainable AI creates trust, encourages <br />   
									ethical conduct, and empowers users through explanations, error detection,<br />     
									and effective human-AI collaboration.</p>   
								</header>
								<div class="image main"><img src="images/opportunity2.jpg" alt="" /></div> 
								<h3>Benefits of Explainable AI</h3>
								<p>1. Transparency & Accountability: The LIME (Local Interpretable Model-agnostic Explanations) framework (Ribeiro et al., 2016), produces interpretable explanations for complicated AI models. It focuses on the significance of interpretability in establishing trust across AI systems through providing concise explanations for specific projected outcomes. This builds trust and accountability in the system's outputs by assisting users and stakeholders in understanding the logic behind AI decisions.</p>
								<p>2. Detecting errors and improving models: Guidotti's research (Guidotti et al., 2018) suggests an overview of approaches for machine learning that can be interpreted regardless of the model used. The study emphasises how XAI approaches can help with detecting errors, and comprehending the AI model’s limitations, enabling advancement of AI models. XAI allows researchers and developers to improve and optimise AI systems, producing more accurate and trustworthy outcomes through providing explanations for model behaviour and potential flaws.</p>
								<div class="image main"><img src="images/opp2.jpg" alt="" /></div>
								<p>3. Identifying Discrimination & Bias: Practitioners and researchers can find discriminatory & bias patterns potentially present in the data by analysing the data used to build XAI models. A study indicates how Inaccurate results and discriminatory practices might result from AI systems that have been trained on extensive datasets that unintentionally reinforce prejudices. It also highlights how crucial it is to recognise these biases and eliminate them in order to achieve morally righteous and egalitarian AI implementations (Manyika et al., 2022) Therefore, XAI would help create more fair & equitable  results in numerous fields, including financing, recruitment, and criminal justice, by recognising and eradicating these biases. </p>
								<p>4. Optimising data selection and curation practises: The study indicates how the effectiveness and results of AI and Machine Learning models are significantly impacted by the quality of the data used to train those systems. In order to guarantee the reliability & accuracy of AI models, it's claimed that data curation—the act of choosing, organising, and cleaning the data—is essential (TagX, 2023). Thus enabling companies to create more thorough and objective AI models by constant observation & selecting diverse and high-quality datasets that accurately reflect the intricacies of real-world circumstances.</p>
								</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<section>
							<h3> References</h3>
							<p>Ribeiro, M. T., Singh, S., & Guestrin, C. (2016, August 9). “why should I trust you?”: Explaining the predictions of any classifier. arXiv.org. https://arxiv.org/abs/1602.04938</p>
							<p>Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Pedreschi, D., &amp; Giannotti, F. (2018, June 21). A survey of methods for explaining Black Box Models. arXiv.org. https://arxiv.org/abs/1802.01933 </p>
							<p>Manyika, J., Presten, B., &amp; Silberg, J. (2022, November 17). What do we do about the biases in ai?. Harvard Business Review. https://hbr.org/2019/10/what-do-we-do-about-the-biases-in-ai </p>
							<p>TagX (2023) Data curation: Key step for AI/ML Data Preparation, Medium. Available at: https://medium.com/@tagxdata/data-curation-key-step-for-ai-ml-data-preparation-694d2c75ffd6 (Accessed: 09 June 2023). </p>
						</section>
					</footer>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; html5up</li></ul>
					</div>

			</div>

					<!-- Scripts -->
					<script src="assets/js/jquery.min.js"></script>
					<script src="assets/js/jquery.scrollex.min.js"></script>
					<script src="assets/js/jquery.scrolly.min.js"></script>
					<script src="assets/js/browser.min.js"></script>
					<script src="assets/js/breakpoints.min.js"></script>
					<script src="assets/js/util.js"></script>
					<script src="assets/js/main.js"></script>
		

	</body>
</html>